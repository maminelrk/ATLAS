cat > ~/atlas_project/test_llm.py << 'EOF'
import sys
import traceback

print("ðŸ” Starting LLM load test...")
print(f"Python version: {sys.version}")

try:
    print("\n1ï¸âƒ£ Importing llama_cpp...")
    from llama_cpp import Llama
    print("âœ… Import successful")
    
    print("\n2ï¸âƒ£ Attempting to load model...")
    llm = Llama(
        model_path="models/gemma-2b-q4_k_m.gguf",
        n_ctx=2048,
        n_threads=4,
        n_batch=512,
        verbose=True
    )
    print("âœ… Model loaded successfully!")
    
    print("\n3ï¸âƒ£ Testing inference...")
    response = llm("Hello", max_tokens=10)
    print(f"âœ… Response: {response}")
    
except Exception as e:
    print(f"\nâŒ ERROR OCCURRED:")
    print(f"Error type: {type(e).__name__}")
    print(f"Error message: {str(e)}")
    print("\nðŸ“‹ Full traceback:")
    traceback.print_exc()
    
    # Try to get more info
    import os
    print(f"\nðŸ“ Model file exists: {os.path.exists('models/gemma-2b-q4_k_m.gguf')}")
    if os.path.exists('models/gemma-2b-q4_k_m.gguf'):
        print(f"ðŸ“Š Model file size: {os.path.getsize('models/gemma-2b-q4_k_m.gguf') / (1024**3):.2f} GB")

print("\nðŸ Test completed")
EOF
